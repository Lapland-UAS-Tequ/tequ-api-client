[{"id":"f5489383c48b7546","type":"subflow","name":"[AI] Detect-acv","info":"Make prediction on image with Tensorflow.js model trained and exported from Microsoft Azure Custom Vision.\n\nInput image must be image buffer in **'msg.payload'**.\n\nModel is loaded from configured folder.\n\nInference image and add result to output message. \n\nCalculates approximation of length in centimeters of detected object(s) based on given **image_width_cm**. \n\nParameter **image_width_cm** can be set in 'settings.js'-file separately for each msg.topic (datasource id).\n\nFor example:\n\n`process.env.image_width_cm = JSON.stringify({\"10\":130,\"11\":130,\"20\":130,\"21\":130});`\n\n`{\n    { msg.topic:image width [cm] },\n    { msg.topic:image width [cm] }\n}`\n\n\nBasic image info and exif is added to output message, if available.\n\nTo train and exmport a model, please look:\n\nhttps://www.customvision.ai/\n","category":"Tequ-API Client","in":[{"x":60,"y":80,"wires":[{"id":"4a3260e906997fa4"}]}],"out":[{"x":980,"y":380,"wires":[{"id":"70f48281642455fa","port":0}]}],"env":[{"name":"model_folder","type":"str","value":"C:\\Users\\juha.autioniemi\\.node-red\\test","ui":{"type":"input","opts":{"types":["str"]}}},{"name":"threshold","type":"num","value":"0.50","ui":{"type":"spinner","opts":{"min":0,"max":1}}},{"name":"image_width_cm","type":"num","value":"130","ui":{"type":"input","opts":{"types":["num"]}}}],"meta":{"module":"node-red-tequ-ai-detect-acv","version":"0.0.1","author":"juha.autioniemi@lapinamk.fi","desc":"Make prediction on image with Tensorflow.js model trained in Microsoft Azure Custom Vision service.","license":"MIT"},"color":"#FFCC66","icon":"node-red/status.svg","status":{"x":980,"y":460,"wires":[{"id":"e220d68131d1bea3","port":0}]}},{"id":"b2e07a6cce1a9b6e","type":"function","z":"f5489383c48b7546","name":"customvision","func":"var imageBuffer = msg.payload;\nvar results = [];\nvar labels = context.get(\"labels\");\nvar threshold = msg.threshold;\nvar image_width = msg.width;\nvar image_height = msg.height;\n\n//Make prediction on input image\nmodel = context.get(\"model\")\nresult = await model.executeAsync(imageBuffer);\n\n\n\n//Map result to JSON and connect label names\nvar bboxes = result[0];\nvar scores = result[1];\nvar label_idxs = result[2];\n\nfor(var i=0;i<bboxes.length;i++){\n    if(scores[i] >= threshold){\n        newResult = {\n            \"bbox\":[\n                bboxes[i][0] * image_width,\n                bboxes[i][1] * image_height,\n                (bboxes[i][2] - bboxes[i][0]) * image_width,\n                (bboxes[i][3] - bboxes[i][1]) * image_height\n            ],\n            \"class\":labels[label_idxs[i]],\n            \"label\":labels[label_idxs[i]],\n            \"score\":scores[i]\n        }\n        results.push(newResult)\n    }\n}\n\n//Calculate object width if image_width_cm is given input message\nif(\"image_width_cm\" in msg){\n    var image_width_cm = msg.image_width_cm;\n\n    for(var j=0;j<results.length;j++){\n        px_in_cm = image_width_cm / msg.width\n        object_size_cm = px_in_cm * results[j].bbox[2]\n        results[j].length_cm = Math.round(object_size_cm)\n    }\n}\n\n// Create output message\nresult_message = {\n    \"start\":msg.start,\n    \"labels\":context.get(\"labels\"),\n    \"originalResult\":result,\n    \"thresholdType\":msg.thresholdType,\n    \"threshold\": msg.threshold,\n    \"image_width_cm\":msg.image_width_cm,\n    \"image_width_cm_type\":msg.image_width_cm_type,\n    \"topic\":msg.topic,\n    \"payload\":{\n        \"inference\":{\n            \"metadata\":context.get(\"metadata\"),\n            \"manifest\":context.get(\"manifest\"),\n            \"time_ms\":0,\n            \"result\":results,\n            \"model\":env.get(\"inference_model_id\"),\n            \"type\":\"object detection\"\n        },\n        \"image\":{\n            \"buffer\":imageBuffer,\n            \"width\": msg.width,\n            \"height\": msg.height,\n            \"type\": msg.type,\n            \"size\": (imageBuffer).length,\n            \"exif\":{}\n        }\n    }\n}\n\n// Add exif information\nif(msg.exif){\n     result_message.payload.image.exif = msg.exif\n}\n\nreturn result_message;","outputs":1,"noerr":0,"initialize":"// Code added here will be run once\n// whenever the node is started.\nplatform = os.platform()\nnode.warn(platform)\n\nif(platform == \"win32\"){\n    model_folder = env.get(\"model_folder\")\n    manifest_file = model_folder+\"\\\\\"+\"cvexport.manifest\"\n}\nelse{\n    model_folder = env.get(\"model_folder\")\n    manifest_file = model_folder+\"/\"+\"cvexport.manifest\"\n}\n\nif (context.get(\"model\") === undefined) {\n    try {\n        context.set(\"manifest\",JSON.parse(fs.readFileSync(manifest_file, 'utf8')))\n    } catch (err) {\n        node.error(err,msg)\n    }\n}\n\nif(platform == \"win32\"){\n    LabelFileName           = model_folder+\"\\\\\"+context.get(\"manifest\")[\"LabelFileName\"]\n    ModelFileName           = \"file://\"+model_folder+\"\\\\\"+\"model.json\"\n    MetadataPropsFileName   = model_folder+\"\\\\\"+context.get(\"manifest\")[\"MetadataPropsFileName\"]\n}\nelse{\n    LabelFileName           = model_folder+\"/\"+context.get(\"manifest\")[\"LabelFileName\"]\n    ModelFileName           = \"file://\"+model_folder+\"/\"+\"model.json\"\n    MetadataPropsFileName   = model_folder+\"/\"+context.get(\"manifest\")[\"MetadataPropsFileName\"]\n}\n\n//node.warn(LabelFileName)\n//node.warn(ModelFileName)\n//node.warn(MetadataPropsFileName)\n\nif (context.get(\"labels\") === undefined) {\n    try {\n        var labels = fs.readFileSync(LabelFileName, 'utf8')\n        labelArray = labels.split(\"\\n\")\n        context.set(\"labels\",labelArray)\n        \n    } catch (err) {\n        node.error(\"Error reading labels\",err)\n    }\n}\n\nif (context.get(\"metadata\") === undefined) {\n    try {\n        context.set(\"metadata\",JSON.parse(fs.readFileSync(MetadataPropsFileName, 'utf8')))\n    } catch (err) {\n        node.error(\"Error reading metadata\",err)\n    }\n}\n\nif (context.get(\"model\") === undefined) {\n    try {\n        node.status({fill:\"green\",shape:\"dot\",text:\"Loading model...\"});\n        const model = new microsoftCustomvisionTfjsNode.ObjectDetectionModel();\n        await model.loadModelAsync(ModelFileName);\n        context.set(\"model\", model)\n        node.status({fill:\"green\",shape:\"dot\",text:\"Object detection ready.\"});\n    } catch (err) {\n        node.status({fill:\"red\",shape:\"dot\",text:err});\n        node.error(\"Error reading model\",err)\n    }\n}\n\nnode.warn(tf.getBackend())\n","finalize":"","libs":[{"var":"microsoftCustomvisionTfjsNode","module":"@microsoft/customvision-tfjs-node"},{"var":"fs","module":"fs"},{"var":"os","module":"os"},{"var":"tf","module":"@tensorflow/tfjs-node-gpu"}],"x":530,"y":380,"wires":[["5f405028d59497da"]]},{"id":"c25b00fd941b95b8","type":"change","z":"f5489383c48b7546","name":"timer","rules":[{"t":"set","p":"start","pt":"msg","to":"","tot":"date"}],"action":"","property":"","from":"","to":"","reg":false,"x":190,"y":160,"wires":[["f93f094154f19d8a"]]},{"id":"5f405028d59497da","type":"change","z":"f5489383c48b7546","name":"end timer","rules":[{"t":"set","p":"payload.inference.time_ms","pt":"msg","to":"$millis() - msg.start","tot":"jsonata"}],"action":"","property":"","from":"","to":"","reg":false,"x":700,"y":380,"wires":[["70f48281642455fa"]]},{"id":"f93f094154f19d8a","type":"image-info","z":"f5489383c48b7546","name":"","x":210,"y":240,"wires":[["9ed9b3aecaea1658"]]},{"id":"9ed9b3aecaea1658","type":"exif","z":"f5489383c48b7546","name":"","mode":"normal","property":"payload","x":190,"y":320,"wires":[["c43ad9454285328e"]]},{"id":"c43ad9454285328e","type":"function","z":"f5489383c48b7546","name":"Set threshold & image_width_cm","func":"//Define threshold\nvar threshold = 0;\n\nglobal_settings = global.get(\"settings\") || undefined\nvar thresholdType = \"\"\n\nif(global_settings !== undefined){\n    if(\"threshold\" in global_settings){\n        threshold = global_settings[\"threshold\"]\n        thresholdType = \"global\";\n    }\n}\n\nelse if(\"threshold\" in msg){\n    threshold = msg.threshold;\n    thresholdType = \"msg\";\n    if(threshold < 0){\n        threshold = 0\n    }\n    else if(threshold > 1){\n        threshold = 1\n    }\n}\n\nelse{\n    try{\n        threshold = env.get(\"threshold\");\n        thresholdType = \"env\";\n    }\n    catch(err){\n        threshold = 0.5\n        thresholdType = \"default\";\n    }\n}\n\n\ntry{\n    image_width_cm_type = \"env\";\n    image_width_cm = JSON.parse(env.get(\"image_width_cm\"))[msg.topic];\n        \n}\ncatch(err){\n    image_width_cm = 130\n    image_width_cm_type = \"default\";\n}\n\n\nif(threshold == undefined){\n    threshold = 0\n}\n\nmsg.thresholdType = thresholdType;\nmsg.threshold = threshold;\nmsg.image_width_cm = image_width_cm;\nmsg.image_width_cm_type = image_width_cm_type;\n//node.status({fill:\"green\",shape:\"dot\",text:\"threshold: \"+threshold+\" | Image width: \"+image_width_cm});\nreturn msg;","outputs":1,"noerr":0,"initialize":"","finalize":"","libs":[],"x":280,"y":380,"wires":[["b2e07a6cce1a9b6e"]]},{"id":"4a3260e906997fa4","type":"function","z":"f5489383c48b7546","name":"isBuffer?","func":"timestamp = new Date().toISOString();\nvar image = msg.payload;\n\nif(Buffer.isBuffer(image)){\n    //node.status({fill:\"green\",shape:\"dot\",text:timestamp + \" OK\"});  \n    return msg;\n}\nelse{\n    node.error(\"msg.payload is not an image buffer\",msg)\n    node.status({fill:\"red\",shape:\"dot\",text:timestamp + \" msg.payload is not an image buffer\"});  \n}","outputs":1,"noerr":0,"initialize":"","finalize":"","libs":[],"x":200,"y":80,"wires":[["c25b00fd941b95b8"]]},{"id":"e220d68131d1bea3","type":"status","z":"f5489383c48b7546","name":"","scope":["b2e07a6cce1a9b6e","70f48281642455fa"],"x":860,"y":460,"wires":[[]]},{"id":"70f48281642455fa","type":"function","z":"f5489383c48b7546","name":"","func":"node.status({fill:\"blue\",shape:\"dot\",text:msg.payload.inference.time_ms+\" ms\"});  \nreturn msg;","outputs":1,"noerr":0,"initialize":"","finalize":"","libs":[],"x":860,"y":380,"wires":[[]]},{"id":"118f9b2fefef307a","type":"subflow","name":"[AI] Detect-v2","info":"Make prediction on image with Tensorflow.js model trained with tequ-tf1-ca-training-pipeline.\n\nInput image must be image buffer in **'msg.payload'**.\n\nModel is loaded from configured folder.\n\nInference image and add result to output message. \n\nCalculates approximation of length in centimeters of detected object(s) based on given **image_width_cm**. \n\nParameter **image_width_cm** can be set in 'settings.js'-file separately for each msg.topic (datasource id).\n\nFor example:\n\n`process.env.image_width_cm = JSON.stringify({\"10\":130,\"11\":130,\"20\":130,\"21\":130});`\n\n`{\n    { msg.topic:image width [cm] },\n    { msg.topic:image width [cm] }\n}`\n\n\nBasic image info and exif is added to output message, if available.\n\nTo train a model, please look:\n\nhttps://github.com/juhaautioniemi/tequ-tf1-ca-training-pipeline\n\n\n\n\n\n\n","category":"Tequ-API Client","in":[{"x":60,"y":80,"wires":[{"id":"a6780e6ae7c67a93"}]}],"out":[{"x":1000,"y":360,"wires":[{"id":"6f79d6a589d648f8","port":0}]}],"env":[{"name":"threshold","type":"num","value":"0.5","ui":{"type":"input","opts":{"types":["num","env"]}}},{"name":"image_width_cm","type":"json","value":"{\"10\":130}","ui":{"type":"input","opts":{"types":["json","env"]}}},{"name":"model_folder","type":"str","value":"C:\\\\Users\\\\juha.autioniemi\\\\.node-red\\\\config\\\\model","ui":{"type":"input","opts":{"types":["str"]}}}],"meta":{"module":"node-red-contrib-tequ-ai-detect-v2","version":"0.0.1","author":"juha.autioniemi@lapinamk.fi","desc":"Detect objects from image using Cloud Annotations JavaScript SDK.","license":"MIT"},"color":"#FFCC66","icon":"node-red/status.svg","status":{"x":1000,"y":420,"wires":[{"id":"367f9f3de6e7b672","port":0}]}},{"id":"1556db751a958148","type":"function","z":"118f9b2fefef307a","name":"model.detect","func":"var imageBuffer = msg.payload;\nvar results = [];\nvar labels = context.get(\"labels\");\nvar threshold = msg.threshold;\nvar image_width = msg.width;\nvar image_height = msg.height;\n\n//Make prediction on input image\nmodel = context.get(\"model\")\nresults = await model.detect(imageBuffer)   \n//Calculate object width if image_width_cm is given input message\nif(\"image_width_cm\" in msg){\n    var image_width_cm = msg.image_width_cm;\n\n    for(var j=0;j<results.length;j++){\n        px_in_cm = image_width_cm / msg.width\n        object_size_cm = px_in_cm * results[j].bbox[2]\n        results[j].length_cm = Math.round(object_size_cm)\n    }\n}\n\n// Create output message\nresult_message = {\n    \"start\":msg.start,\n    \"labels\":context.get(\"labels\"),\n    \"thresholdType\":msg.thresholdType,\n    \"threshold\": msg.threshold,\n    \"image_width_cm\":msg.image_width_cm,\n    \"image_width_cm_type\":msg.image_width_cm_type,\n    \"topic\":msg.topic,\n    \"payload\":{\n        \"inference\":{\n            \"metadata\":{\n                \"format\":context.get(\"model_file\")[\"format\"],\n                \"generatedBy\":context.get(\"model_file\")[\"generatedBy\"],\n                \"convertedBy\":context.get(\"model_file\")[\"convertedBy\"],\n            },\n            \"time_ms\":0,\n            \"result\":results,\n            \"model\":env.get(\"inference_model_id\"),\n            \"type\":\"object detection\"\n        },\n        \"image\":{\n            \"buffer\":imageBuffer,\n            \"width\": msg.width,\n            \"height\": msg.height,\n            \"type\": msg.type,\n            \"size\": (imageBuffer).length,\n            \"exif\":{}\n        }\n    }\n}\n\n// Add exif information\nif(msg.exif){\n     result_message.payload.image.exif = msg.exif\n}\n\nreturn result_message;","outputs":1,"noerr":0,"initialize":"// Code added here will be run once\n// whenever the node is started.\nplatform = os.platform()\nmodel_folder = env.get(\"model_folder\")\n\nif(platform == \"win32\"){\n    model_file = model_folder+\"\\\\\"+\"model.json\"\n    labels_file = model_folder+\"\\\\\"+\"labels.json\"    \n}\nelse{\n    model_file = model_folder+\"/\"+\"model.json\"\n    labels_file = model_folder+\"/\"+\"labels.json\"        \n}\n\ntry {\n        if(fs.existsSync(model_folder)){\n            if(fs.existsSync(model_file)){\n                if(fs.existsSync(labels_file)){\n                    context.set(\"model_file\",JSON.parse(fs.readFileSync(model_file, 'utf8')))\n                    context.set(\"labels\",JSON.parse(fs.readFileSync(labels_file, 'utf8')))\n                    const model = await models.load(model_folder)\n                    context.set(\"model\",model)\n                    node.status({fill:\"green\",shape:\"dot\",text:\"Model ready\"})\n                }\n                else{\n                    node.status({fill:\"red\",shape:\"dot\",text:\"labels.json not found\"})        \n                }\n            }\n            else{\n                node.status({fill:\"red\",shape:\"dot\",text:\"models.json not found\"})    \n            }\n        }\n        else{\n            node.status({fill:\"red\",shape:\"dot\",text:\"Model folder not found\"})\n        }\n}\ncatch (err) {\n        node.status({fill:\"red\",shape:\"dot\",text:\"Error loading model\"})\n        node.error(err,msg)\n}","finalize":"","libs":[{"var":"models","module":"@cloud-annotations/models-node-gpu"},{"var":"fs","module":"fs"},{"var":"os","module":"os"}],"x":530,"y":360,"wires":[["a635567c4613328a"]]},{"id":"625bb41e8b68ee34","type":"change","z":"118f9b2fefef307a","name":"timer","rules":[{"t":"set","p":"start","pt":"msg","to":"","tot":"date"}],"action":"","property":"","from":"","to":"","reg":false,"x":170,"y":160,"wires":[["0733b75c79be4452"]]},{"id":"0733b75c79be4452","type":"image-info","z":"118f9b2fefef307a","name":"","x":190,"y":220,"wires":[["6a02acaed2d97ceb"]]},{"id":"6a02acaed2d97ceb","type":"exif","z":"118f9b2fefef307a","name":"","mode":"normal","property":"payload","x":170,"y":300,"wires":[["24b022bea4a2b9ad"]]},{"id":"24b022bea4a2b9ad","type":"function","z":"118f9b2fefef307a","name":"Set threshold & image_width_cm","func":"//Define threshold\nvar threshold = 0;\n\nglobal_settings = global.get(\"settings\") || undefined\nvar thresholdType = \"\"\n\nif(global_settings !== undefined){\n    if(\"threshold\" in global_settings){\n        threshold = global_settings[\"threshold\"]\n        thresholdType = \"global\";\n    }\n}\n\nelse if(\"threshold\" in msg){\n    threshold = msg.threshold;\n    thresholdType = \"msg\";\n    if(threshold < 0){\n        threshold = 0\n    }\n    else if(threshold > 1){\n        threshold = 1\n    }\n}\n\nelse{\n    try{\n        threshold = env.get(\"threshold\");\n        thresholdType = \"env\";\n    }\n    catch(err){\n        threshold = 0.5\n        thresholdType = \"default\";\n    }\n    \n    \n   \n}\n\n\ntry{\n    image_width_cm_type = \"env\";\n    image_width_cm = JSON.parse(env.get(\"image_width_cm\"))[msg.topic];\n        \n}\ncatch(err){\n    image_width_cm = 130\n    image_width_cm_type = \"default\";\n}\n\n\nif(threshold == undefined){\n    threshold = 0\n}\n\nmsg.thresholdType = thresholdType;\nmsg.threshold = threshold;\nmsg.image_width_cm = image_width_cm;\nmsg.image_width_cm_type = image_width_cm_type;\n//node.status({fill:\"green\",shape:\"dot\",text:\"threshold: \"+threshold+\" | Image width: \"+image_width_cm});\nreturn msg;","outputs":1,"noerr":0,"initialize":"","finalize":"","libs":[],"x":260,"y":360,"wires":[["1556db751a958148"]]},{"id":"a6780e6ae7c67a93","type":"function","z":"118f9b2fefef307a","name":"isBuffer?","func":"timestamp = new Date().toISOString();\nvar image = msg.payload;\n\nif(Buffer.isBuffer(image)){\n    return msg;\n}\nelse{\n    node.error(\"msg.payload is not an image buffer\",msg)\n    node.status({fill:\"red\",shape:\"dot\",text:timestamp + \" msg.payload is not an image buffer\"});  \n}","outputs":1,"noerr":0,"initialize":"","finalize":"","libs":[],"x":180,"y":80,"wires":[["625bb41e8b68ee34"]]},{"id":"a635567c4613328a","type":"change","z":"118f9b2fefef307a","name":"end timer","rules":[{"t":"set","p":"payload.inference.time_ms","pt":"msg","to":"$millis() - msg.start","tot":"jsonata"}],"action":"","property":"","from":"","to":"","reg":false,"x":700,"y":360,"wires":[["6f79d6a589d648f8"]]},{"id":"6f79d6a589d648f8","type":"function","z":"118f9b2fefef307a","name":"","func":"node.status({fill:\"blue\",shape:\"dot\",text:msg.payload.inference.time_ms+\" ms\"});  \nreturn msg;","outputs":1,"noerr":0,"initialize":"","finalize":"","libs":[],"x":860,"y":360,"wires":[[]]},{"id":"367f9f3de6e7b672","type":"status","z":"118f9b2fefef307a","name":"","scope":["6f79d6a589d648f8","a6780e6ae7c67a93","1556db751a958148"],"x":860,"y":420,"wires":[[]]},{"id":"d6f94a991ea84f26","type":"subflow:f5489383c48b7546","z":"118f9b2fefef307a","name":"","env":[],"x":250,"y":560,"wires":[[]]},{"id":"6ce80a2a2b607dad","type":"subflow:118f9b2fefef307a","z":"d994f01a5b816cf9","name":"","env":[],"x":670,"y":1600,"wires":[[]]}]